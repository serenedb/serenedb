

#ifdef __AVX2__

/** avxpacking with delta encoding **/
#include <immintrin.h>
#include <stdint.h>
#include <string.h>
#include <simdcomputil.h>
#include <avxdeltabitpacking.h>



static inline uint32_t avxmaxbitas32int(__m256i accumulator) {
    __m128i lo = _mm256_castsi256_si128(accumulator);
    __m128i hi = _mm256_extracti128_si256(accumulator, 1);
    __m128i combined = _mm_or_si128(lo, hi);
    combined = _mm_or_si128(combined, _mm_srli_si128(combined, 8));
    combined = _mm_or_si128(combined, _mm_srli_si128(combined, 4));
    return bits(_mm_cvtsi128_si32(combined));
}

/* Compute max bits needed for delta-encoded block (SIMD version) */
uint32_t avxmaxbitsd1(uint32_t initvalue, const uint32_t *in) {
    const __m256i *pin = (const __m256i *)in;
    __m256i prev_vec = _mm256_set1_epi32(initvalue);
    __m256i newvec = _mm256_loadu_si256(pin++);
    __m256i accumulator = avx_delta(newvec, prev_vec);
    prev_vec = newvec;

    for (uint32_t k = 1; k < 32; ++k) {
        newvec = _mm256_loadu_si256(pin++);
        accumulator = _mm256_or_si256(accumulator, avx_delta(newvec, prev_vec));
        prev_vec = newvec;
    }
    return avxmaxbitas32int(accumulator);
}


static void avxipackblock0(const uint32_t * pin, __m256i * compressed, uint32_t prev) {
  (void)compressed;
  (void)prev;
  (void) pin; /* we consumed 256 32-bit integers */ 
}


/* we are going to pack 256 1-bit values, touching 1 256-bit words, using 16 bytes */ 
static void avxipackblock1(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  1 256-bit word */ 
  __m256i w0;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 11));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 13));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 15));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 17));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 19));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 21));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 22));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 23));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 25));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 26));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 27));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 28));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 29));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 30));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 31));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 2-bit values, touching 2 256-bit words, using 32 bytes */ 
static void avxipackblock2(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  2 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 22));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 26));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 28));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 30));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 22));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 26));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 28));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 30));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 3-bit values, touching 3 256-bit words, using 48 bytes */ 
static void avxipackblock3(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  3 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 15));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 21));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 27));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 13));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 19));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 22));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 25));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 28));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 31));
  w0 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 11));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 17));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 23));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 26));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 29));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 4-bit values, touching 4 256-bit words, using 64 bytes */ 
static void avxipackblock4(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  4 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 28));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 28));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 28));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 28));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 5-bit values, touching 5 256-bit words, using 80 bytes */ 
static void avxipackblock5(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  5 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 15));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 25));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 13));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 23));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 11));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 21));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 26));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 31));
  w1 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 19));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 29));
  w0 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 17));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 22));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 27));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 6-bit values, touching 6 256-bit words, using 96 bytes */ 
static void avxipackblock6(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  6 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 22));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 26));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 22));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 26));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 7-bit values, touching 7 256-bit words, using 112 bytes */ 
static void avxipackblock7(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  7 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 21));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 17));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 24));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 31));
  w0 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 13));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 27));
  w1 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 23));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 19));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 15));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 22));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 29));
  w0 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 11));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 25));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 8-bit values, touching 8 256-bit words, using 128 bytes */ 
static void avxipackblock8(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  8 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 24));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 24));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 24));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 24));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 24));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 24));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 24));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 24));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 9-bit values, touching 9 256-bit words, using 144 bytes */ 
static void avxipackblock9(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  9 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 27));
  w1 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 13));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 22));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 31));
  w0 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 17));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 21));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 25));
  w1 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 11));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 29));
  w0 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 15));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 19));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 23));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 10-bit values, touching 10 256-bit words, using 160 bytes */ 
static void avxipackblock10(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  10 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 22));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 22));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 11-bit values, touching 11 256-bit words, using 176 bytes */ 
static void avxipackblock11(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  11 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 11));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 23));
  w0 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 13));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 25));
  w0 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 15));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 27));
  w0 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 17));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 29));
  w0 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 19));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 20));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 31));
  w0 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 21));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 12-bit values, touching 12 256-bit words, using 192 bytes */ 
static void avxipackblock12(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  12 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 20));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 20));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 20));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 13-bit values, touching 13 256-bit words, using 208 bytes */ 
static void avxipackblock13(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  13 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 13));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 27));
  w1 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 21));
  w0 = _mm256_srli_epi32(delta,11);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 15));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 22));
  w0 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 29));
  w1 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 23));
  w0 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 17));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 11));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 18));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 31));
  w1 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 25));
  w0 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 19));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 14-bit values, touching 14 256-bit words, using 224 bytes */ 
static void avxipackblock14(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  14 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 22));
  w0 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 18));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 18));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 15-bit values, touching 15 256-bit words, using 240 bytes */ 
static void avxipackblock15(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  15 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 15));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 13));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 11));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 18));
  w1 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 31));
  w0 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 29));
  w1 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 27));
  w0 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 25));
  w1 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 23));
  w0 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 21));
  w1 = _mm256_srli_epi32(delta,11);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 19));
  w0 = _mm256_srli_epi32(delta,13);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 17));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 16-bit values, touching 16 256-bit words, using 256 bytes */ 
static void avxipackblock16(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  16 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 16));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 17-bit values, touching 17 256-bit words, using 272 bytes */ 
static void avxipackblock17(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  17 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 17));
  w1 = _mm256_srli_epi32(delta,15);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 19));
  w0 = _mm256_srli_epi32(delta,13);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 21));
  w1 = _mm256_srli_epi32(delta,11);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 23));
  w0 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 25));
  w1 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 27));
  w0 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 29));
  w1 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 14));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 31));
  w0 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 18));
  w0 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 22));
  w0 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 11));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 13));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 15));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 18-bit values, touching 18 256-bit words, using 288 bytes */ 
static void avxipackblock18(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  18 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 18));
  w1 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 22));
  w0 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 14));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 18));
  w0 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 14));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 19-bit values, touching 19 256-bit words, using 304 bytes */ 
static void avxipackblock19(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  19 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 19));
  w1 = _mm256_srli_epi32(delta,13);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 25));
  w0 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 31));
  w1 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 18));
  w0 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 11));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 17));
  w1 = _mm256_srli_epi32(delta,15);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 23));
  w0 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 29));
  w1 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 15));
  w1 = _mm256_srli_epi32(delta,17);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 21));
  w0 = _mm256_srli_epi32(delta,11);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 27));
  w1 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 14));
  w0 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 13));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 20-bit values, touching 20 256-bit words, using 320 bytes */ 
static void avxipackblock20(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  20 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 12));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 12));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 21-bit values, touching 21 256-bit words, using 336 bytes */ 
static void avxipackblock21(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  21 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 21));
  w1 = _mm256_srli_epi32(delta,11);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 31));
  w0 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 9));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 19));
  w1 = _mm256_srli_epi32(delta,13);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 29));
  w0 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 18));
  w1 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 17));
  w1 = _mm256_srli_epi32(delta,15);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 27));
  w0 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 15));
  w1 = _mm256_srli_epi32(delta,17);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 25));
  w0 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 14));
  w1 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 13));
  w1 = _mm256_srli_epi32(delta,19);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 23));
  w0 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 12));
  w1 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 22));
  w0 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 11));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 22-bit values, touching 22 256-bit words, using 352 bytes */ 
static void avxipackblock22(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  22 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 12));
  w0 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 14));
  w0 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 18));
  w0 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 10));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 22));
  w0 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 12));
  w1 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 14));
  w1 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 18));
  w1 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 10));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 23-bit values, touching 23 256-bit words, using 368 bytes */ 
static void avxipackblock23(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  23 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 23));
  w1 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 14));
  w0 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 19));
  w0 = _mm256_srli_epi32(delta,13);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 10));
  w1 = _mm256_srli_epi32(delta,22);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 15));
  w1 = _mm256_srli_epi32(delta,17);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 29));
  w0 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 11));
  w0 = _mm256_srli_epi32(delta,21);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 25));
  w1 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 7));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 21));
  w0 = _mm256_srli_epi32(delta,11);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 12));
  w1 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 17));
  w1 = _mm256_srli_epi32(delta,15);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 31));
  w0 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 13));
  w0 = _mm256_srli_epi32(delta,19);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 27));
  w1 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 18));
  w0 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 9));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 24-bit values, touching 24 256-bit words, using 384 bytes */ 
static void avxipackblock24(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  24 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 8));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 8));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 25-bit values, touching 25 256-bit words, using 400 bytes */ 
static void avxipackblock25(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  25 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 25));
  w1 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 18));
  w0 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 11));
  w1 = _mm256_srli_epi32(delta,21);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 29));
  w0 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 15));
  w0 = _mm256_srli_epi32(delta,17);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 8));
  w1 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 19));
  w1 = _mm256_srli_epi32(delta,13);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 12));
  w0 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 5));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 23));
  w0 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 9));
  w0 = _mm256_srli_epi32(delta,23);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 27));
  w1 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 13));
  w1 = _mm256_srli_epi32(delta,19);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 31));
  w0 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 17));
  w0 = _mm256_srli_epi32(delta,15);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 10));
  w1 = _mm256_srli_epi32(delta,22);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 21));
  w1 = _mm256_srli_epi32(delta,11);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 14));
  w0 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 7));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 26-bit values, touching 26 256-bit words, using 416 bytes */ 
static void avxipackblock26(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  26 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 14));
  w1 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 8));
  w0 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 22));
  w0 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 10));
  w0 = _mm256_srli_epi32(delta,22);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 18));
  w1 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 12));
  w0 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 6));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 14));
  w0 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 8));
  w1 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 10));
  w1 = _mm256_srli_epi32(delta,22);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 18));
  w0 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 12));
  w1 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 6));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 27-bit values, touching 27 256-bit words, using 432 bytes */ 
static void avxipackblock27(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  27 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 27));
  w1 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 22));
  w0 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 17));
  w1 = _mm256_srli_epi32(delta,15);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 12));
  w0 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 7));
  w1 = _mm256_srli_epi32(delta,25);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 29));
  w0 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 19));
  w0 = _mm256_srli_epi32(delta,13);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 14));
  w1 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 9));
  w0 = _mm256_srli_epi32(delta,23);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 31));
  w1 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 21));
  w1 = _mm256_srli_epi32(delta,11);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 11));
  w1 = _mm256_srli_epi32(delta,21);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 6));
  w0 = _mm256_srli_epi32(delta,26);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 23));
  w0 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 18));
  w1 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 13));
  w0 = _mm256_srli_epi32(delta,19);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 8));
  w1 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 3));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 25));
  w1 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 15));
  w1 = _mm256_srli_epi32(delta,17);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 10));
  w0 = _mm256_srli_epi32(delta,22);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 5));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 28-bit values, touching 28 256-bit words, using 448 bytes */ 
static void avxipackblock28(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  28 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 12));
  w1 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 8));
  w0 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 12));
  w0 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 8));
  w1 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 12));
  w1 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 8));
  w0 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 4));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 12));
  w0 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 8));
  w1 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 4));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 29-bit values, touching 29 256-bit words, using 464 bytes */ 
static void avxipackblock29(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  29 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 29));
  w1 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 23));
  w1 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 17));
  w1 = _mm256_srli_epi32(delta,15);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 14));
  w0 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 11));
  w1 = _mm256_srli_epi32(delta,21);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 8));
  w0 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 5));
  w1 = _mm256_srli_epi32(delta,27);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 31));
  w0 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 25));
  w0 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 19));
  w0 = _mm256_srli_epi32(delta,13);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 13));
  w0 = _mm256_srli_epi32(delta,19);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 10));
  w1 = _mm256_srli_epi32(delta,22);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 7));
  w0 = _mm256_srli_epi32(delta,25);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 4));
  w1 = _mm256_srli_epi32(delta,28);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 1));
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 27));
  w1 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 21));
  w1 = _mm256_srli_epi32(delta,11);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 18));
  w0 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 15));
  w1 = _mm256_srli_epi32(delta,17);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 12));
  w0 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 9));
  w1 = _mm256_srli_epi32(delta,23);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 6));
  w0 = _mm256_srli_epi32(delta,26);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 3));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 30-bit values, touching 30 256-bit words, using 480 bytes */ 
static void avxipackblock30(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  30 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 30));
  w1 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 26));
  w1 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 22));
  w1 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 18));
  w1 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 14));
  w1 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 12));
  w0 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 10));
  w1 = _mm256_srli_epi32(delta,22);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 8));
  w0 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 6));
  w1 = _mm256_srli_epi32(delta,26);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 4));
  w0 = _mm256_srli_epi32(delta,28);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 2));
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 28));
  w1 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 24));
  w1 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 22));
  w0 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 20));
  w1 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 18));
  w0 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 16));
  w1 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 14));
  w0 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 12));
  w1 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 10));
  w0 = _mm256_srli_epi32(delta,22);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 8));
  w1 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 6));
  w0 = _mm256_srli_epi32(delta,26);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 4));
  w1 = _mm256_srli_epi32(delta,28);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta , 2));
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}


/* we are going to pack 256 31-bit values, touching 31 256-bit words, using 496 bytes */ 
static void avxipackblock31(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  31 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 31));
  w1 = _mm256_srli_epi32(delta,1);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 30));
  w0 = _mm256_srli_epi32(delta,2);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 29));
  w1 = _mm256_srli_epi32(delta,3);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 28));
  w0 = _mm256_srli_epi32(delta,4);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 27));
  w1 = _mm256_srli_epi32(delta,5);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 26));
  w0 = _mm256_srli_epi32(delta,6);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 25));
  w1 = _mm256_srli_epi32(delta,7);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 24));
  w0 = _mm256_srli_epi32(delta,8);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 23));
  w1 = _mm256_srli_epi32(delta,9);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 22));
  w0 = _mm256_srli_epi32(delta,10);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 21));
  w1 = _mm256_srli_epi32(delta,11);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 20));
  w0 = _mm256_srli_epi32(delta,12);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 19));
  w1 = _mm256_srli_epi32(delta,13);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 18));
  w0 = _mm256_srli_epi32(delta,14);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 17));
  w1 = _mm256_srli_epi32(delta,15);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 16));
  w0 = _mm256_srli_epi32(delta,16);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 15));
  w1 = _mm256_srli_epi32(delta,17);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 14));
  w0 = _mm256_srli_epi32(delta,18);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 13));
  w1 = _mm256_srli_epi32(delta,19);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 12));
  w0 = _mm256_srli_epi32(delta,20);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 11));
  w1 = _mm256_srli_epi32(delta,21);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 10));
  w0 = _mm256_srli_epi32(delta,22);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 9));
  w1 = _mm256_srli_epi32(delta,23);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 8));
  w0 = _mm256_srli_epi32(delta,24);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 7));
  w1 = _mm256_srli_epi32(delta,25);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 6));
  w0 = _mm256_srli_epi32(delta,26);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 5));
  w1 = _mm256_srli_epi32(delta,27);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 4));
  w0 = _mm256_srli_epi32(delta,28);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta, 3));
  w1 = _mm256_srli_epi32(delta,29);
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = _mm256_or_si256(w1,_mm256_slli_epi32(delta, 2));
  w0 = _mm256_srli_epi32(delta,30);
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = _mm256_or_si256(w0,_mm256_slli_epi32(delta , 1));
  prev = curr;
  _mm256_storeu_si256(compressed, w0);
}


/* we are going to pack 256 32-bit values, touching 32 256-bit words, using 512 bytes */ 
static void avxipackblock32(const uint32_t * pin, __m256i * compressed, uint32_t init_value) {
  const __m256i * in = (const __m256i *)  pin;
  /* we are going to touch  32 256-bit words */ 
  __m256i w0, w1;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i curr, delta;
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w1);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w0 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed++, w0);
  curr = _mm256_lddqu_si256(in++);
  delta = avx_delta(curr, prev);
  w1 = delta;
  prev = curr;
  _mm256_storeu_si256(compressed, w1);
}

static void avxiunpackblock0(const __m256i * compressed, uint32_t * pout, uint32_t prev) {
  (void) compressed;
  (void) prev;
  memset(pout,0,256);
}


/* we packed 256 1-bit values, touching 1 256-bit words, using 16 bytes */ 
static void avxiunpackblock1(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  1 256-bit word */ 
  __m256i w0;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(1);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 11) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 13) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 15) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 17) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 19) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 21) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 22) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 23) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 25) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 26) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 27) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 28) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 29) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 30) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 31) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 2-bit values, touching 2 256-bit words, using 32 bytes */ 
static void avxiunpackblock2(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  2 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(3);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 22) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 26) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 28) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 30) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 22) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 26) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 28) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 30) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 3-bit values, touching 3 256-bit words, using 48 bytes */ 
static void avxiunpackblock3(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  3 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(7);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 15) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 21) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 27) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 13) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 19) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 22) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 25) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 28) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 31) ,_mm256_slli_epi32( w0 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 11) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 17) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 23) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 26) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 29) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 4-bit values, touching 4 256-bit words, using 64 bytes */ 
static void avxiunpackblock4(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  4 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(15);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 28) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 28) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 28) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 28) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 5-bit values, touching 5 256-bit words, using 80 bytes */ 
static void avxiunpackblock5(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  5 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(31);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 15) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 25) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 13) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 23) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 11) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 21) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 26) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 31) ,_mm256_slli_epi32( w1 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 19) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 29) ,_mm256_slli_epi32( w0 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 17) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 22) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 27) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 6-bit values, touching 6 256-bit words, using 96 bytes */ 
static void avxiunpackblock6(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  6 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(63);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 22) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 26) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 22) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 26) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 7-bit values, touching 7 256-bit words, using 112 bytes */ 
static void avxiunpackblock7(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  7 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(127);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 21) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 17) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 24) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 31) ,_mm256_slli_epi32( w0 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 13) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 27) ,_mm256_slli_epi32( w1 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 23) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 19) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 15) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 22) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 29) ,_mm256_slli_epi32( w0 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 11) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 25) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 8-bit values, touching 8 256-bit words, using 128 bytes */ 
static void avxiunpackblock8(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  8 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(255);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 24) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 24) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 24) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 24) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 24) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 24) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 24) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 24) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 9-bit values, touching 9 256-bit words, using 144 bytes */ 
static void avxiunpackblock9(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  9 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(511);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 27) ,_mm256_slli_epi32( w1 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 13) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 22) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 31) ,_mm256_slli_epi32( w0 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 17) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 21) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 25) ,_mm256_slli_epi32( w1 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 11) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 29) ,_mm256_slli_epi32( w0 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 15) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 19) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 23) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 10-bit values, touching 10 256-bit words, using 160 bytes */ 
static void avxiunpackblock10(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  10 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(1023);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 22) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 22) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 11-bit values, touching 11 256-bit words, using 176 bytes */ 
static void avxiunpackblock11(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  11 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(2047);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 11) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 23) ,_mm256_slli_epi32( w0 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 13) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 25) ,_mm256_slli_epi32( w0 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 15) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 27) ,_mm256_slli_epi32( w0 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 17) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 29) ,_mm256_slli_epi32( w0 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 19) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 20) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 31) ,_mm256_slli_epi32( w0 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 21) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 12-bit values, touching 12 256-bit words, using 192 bytes */ 
static void avxiunpackblock12(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  12 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(4095);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 20) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 20) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 20) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 20) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 13-bit values, touching 13 256-bit words, using 208 bytes */ 
static void avxiunpackblock13(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  13 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(8191);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 13) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 27) ,_mm256_slli_epi32( w1 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 21) ,_mm256_slli_epi32( w0 , 11 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 15) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 22) ,_mm256_slli_epi32( w0 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 29) ,_mm256_slli_epi32( w1 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 23) ,_mm256_slli_epi32( w0 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 17) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 11) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 18) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 31) ,_mm256_slli_epi32( w1 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 25) ,_mm256_slli_epi32( w0 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 19) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 14-bit values, touching 14 256-bit words, using 224 bytes */ 
static void avxiunpackblock14(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  14 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(16383);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 22) ,_mm256_slli_epi32( w0 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 18) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 18) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 15-bit values, touching 15 256-bit words, using 240 bytes */ 
static void avxiunpackblock15(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  15 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(32767);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 15) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 13) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 11) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 18) ,_mm256_slli_epi32( w1 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 16) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 31) ,_mm256_slli_epi32( w0 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 29) ,_mm256_slli_epi32( w1 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 27) ,_mm256_slli_epi32( w0 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 25) ,_mm256_slli_epi32( w1 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 23) ,_mm256_slli_epi32( w0 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 21) ,_mm256_slli_epi32( w1 , 11 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 19) ,_mm256_slli_epi32( w0 , 13 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 17) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 16-bit values, touching 16 256-bit words, using 256 bytes */ 
static void avxiunpackblock16(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  16 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(65535);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 16) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 17-bit values, touching 17 256-bit words, using 272 bytes */ 
static void avxiunpackblock17(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  17 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(131071);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 17) ,_mm256_slli_epi32( w1 , 15 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 19) ,_mm256_slli_epi32( w0 , 13 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 21) ,_mm256_slli_epi32( w1 , 11 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 23) ,_mm256_slli_epi32( w0 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 25) ,_mm256_slli_epi32( w1 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 27) ,_mm256_slli_epi32( w0 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 29) ,_mm256_slli_epi32( w1 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 14) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 31) ,_mm256_slli_epi32( w0 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 18) ,_mm256_slli_epi32( w0 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 22) ,_mm256_slli_epi32( w0 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 11) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 13) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 15) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 18-bit values, touching 18 256-bit words, using 288 bytes */ 
static void avxiunpackblock18(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  18 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(262143);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 18) ,_mm256_slli_epi32( w1 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 22) ,_mm256_slli_epi32( w0 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 14) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 18) ,_mm256_slli_epi32( w0 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 14) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 19-bit values, touching 19 256-bit words, using 304 bytes */ 
static void avxiunpackblock19(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  19 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(524287);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 19) ,_mm256_slli_epi32( w1 , 13 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 25) ,_mm256_slli_epi32( w0 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 12) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 31) ,_mm256_slli_epi32( w1 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 18) ,_mm256_slli_epi32( w0 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 11) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 17) ,_mm256_slli_epi32( w1 , 15 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 23) ,_mm256_slli_epi32( w0 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 29) ,_mm256_slli_epi32( w1 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 15) ,_mm256_slli_epi32( w1 , 17 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 21) ,_mm256_slli_epi32( w0 , 11 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 27) ,_mm256_slli_epi32( w1 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 14) ,_mm256_slli_epi32( w0 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 13) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 20-bit values, touching 20 256-bit words, using 320 bytes */ 
static void avxiunpackblock20(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  20 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(1048575);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 12) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 12) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 12) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 12) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 21-bit values, touching 21 256-bit words, using 336 bytes */ 
static void avxiunpackblock21(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  21 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(2097151);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 21) ,_mm256_slli_epi32( w1 , 11 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 10) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 31) ,_mm256_slli_epi32( w0 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 9) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 19) ,_mm256_slli_epi32( w1 , 13 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 29) ,_mm256_slli_epi32( w0 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 18) ,_mm256_slli_epi32( w1 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 17) ,_mm256_slli_epi32( w1 , 15 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 27) ,_mm256_slli_epi32( w0 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 15) ,_mm256_slli_epi32( w1 , 17 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 25) ,_mm256_slli_epi32( w0 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 14) ,_mm256_slli_epi32( w1 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 13) ,_mm256_slli_epi32( w1 , 19 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 23) ,_mm256_slli_epi32( w0 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 12) ,_mm256_slli_epi32( w1 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 22) ,_mm256_slli_epi32( w0 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 11) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 22-bit values, touching 22 256-bit words, using 352 bytes */ 
static void avxiunpackblock22(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  22 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(4194303);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 12) ,_mm256_slli_epi32( w0 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 14) ,_mm256_slli_epi32( w0 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 18) ,_mm256_slli_epi32( w0 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 10) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 22) ,_mm256_slli_epi32( w0 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 12) ,_mm256_slli_epi32( w1 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 14) ,_mm256_slli_epi32( w1 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 18) ,_mm256_slli_epi32( w1 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 10) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 23-bit values, touching 23 256-bit words, using 368 bytes */ 
static void avxiunpackblock23(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  23 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(8388607);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 23) ,_mm256_slli_epi32( w1 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 14) ,_mm256_slli_epi32( w0 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 19) ,_mm256_slli_epi32( w0 , 13 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 10) ,_mm256_slli_epi32( w1 , 22 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 15) ,_mm256_slli_epi32( w1 , 17 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 29) ,_mm256_slli_epi32( w0 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 11) ,_mm256_slli_epi32( w0 , 21 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 25) ,_mm256_slli_epi32( w1 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 7) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 21) ,_mm256_slli_epi32( w0 , 11 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 12) ,_mm256_slli_epi32( w1 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 17) ,_mm256_slli_epi32( w1 , 15 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 8) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 31) ,_mm256_slli_epi32( w0 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 13) ,_mm256_slli_epi32( w0 , 19 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 27) ,_mm256_slli_epi32( w1 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 18) ,_mm256_slli_epi32( w0 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 9) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 24-bit values, touching 24 256-bit words, using 384 bytes */ 
static void avxiunpackblock24(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  24 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(16777215);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 8) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 8) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 8) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 8) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 8) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 8) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 8) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 8) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 25-bit values, touching 25 256-bit words, using 400 bytes */ 
static void avxiunpackblock25(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  25 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(33554431);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 25) ,_mm256_slli_epi32( w1 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 18) ,_mm256_slli_epi32( w0 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 11) ,_mm256_slli_epi32( w1 , 21 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 29) ,_mm256_slli_epi32( w0 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 15) ,_mm256_slli_epi32( w0 , 17 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 8) ,_mm256_slli_epi32( w1 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 19) ,_mm256_slli_epi32( w1 , 13 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 12) ,_mm256_slli_epi32( w0 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 5) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 23) ,_mm256_slli_epi32( w0 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 9) ,_mm256_slli_epi32( w0 , 23 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 27) ,_mm256_slli_epi32( w1 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 13) ,_mm256_slli_epi32( w1 , 19 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 6) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 31) ,_mm256_slli_epi32( w0 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 17) ,_mm256_slli_epi32( w0 , 15 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 10) ,_mm256_slli_epi32( w1 , 22 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 21) ,_mm256_slli_epi32( w1 , 11 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 14) ,_mm256_slli_epi32( w0 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 7) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 26-bit values, touching 26 256-bit words, using 416 bytes */ 
static void avxiunpackblock26(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  26 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(67108863);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 14) ,_mm256_slli_epi32( w1 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 8) ,_mm256_slli_epi32( w0 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 22) ,_mm256_slli_epi32( w0 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 10) ,_mm256_slli_epi32( w0 , 22 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 18) ,_mm256_slli_epi32( w1 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 12) ,_mm256_slli_epi32( w0 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 6) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 14) ,_mm256_slli_epi32( w0 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 8) ,_mm256_slli_epi32( w1 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 10) ,_mm256_slli_epi32( w1 , 22 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 18) ,_mm256_slli_epi32( w0 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 12) ,_mm256_slli_epi32( w1 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 6) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 27-bit values, touching 27 256-bit words, using 432 bytes */ 
static void avxiunpackblock27(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  27 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(134217727);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 27) ,_mm256_slli_epi32( w1 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 22) ,_mm256_slli_epi32( w0 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 17) ,_mm256_slli_epi32( w1 , 15 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 12) ,_mm256_slli_epi32( w0 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 7) ,_mm256_slli_epi32( w1 , 25 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 29) ,_mm256_slli_epi32( w0 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 19) ,_mm256_slli_epi32( w0 , 13 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 14) ,_mm256_slli_epi32( w1 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 9) ,_mm256_slli_epi32( w0 , 23 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 4) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 31) ,_mm256_slli_epi32( w1 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 21) ,_mm256_slli_epi32( w1 , 11 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 11) ,_mm256_slli_epi32( w1 , 21 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 6) ,_mm256_slli_epi32( w0 , 26 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w0 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 23) ,_mm256_slli_epi32( w0 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 18) ,_mm256_slli_epi32( w1 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 13) ,_mm256_slli_epi32( w0 , 19 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 8) ,_mm256_slli_epi32( w1 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 3) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 25) ,_mm256_slli_epi32( w1 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 15) ,_mm256_slli_epi32( w1 , 17 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 10) ,_mm256_slli_epi32( w0 , 22 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 5) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 28-bit values, touching 28 256-bit words, using 448 bytes */ 
static void avxiunpackblock28(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  28 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(268435455);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 12) ,_mm256_slli_epi32( w1 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 8) ,_mm256_slli_epi32( w0 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 4) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 12) ,_mm256_slli_epi32( w0 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 8) ,_mm256_slli_epi32( w1 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 4) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 12) ,_mm256_slli_epi32( w1 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 8) ,_mm256_slli_epi32( w0 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 4) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 12) ,_mm256_slli_epi32( w0 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 8) ,_mm256_slli_epi32( w1 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 4) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 29-bit values, touching 29 256-bit words, using 464 bytes */ 
static void avxiunpackblock29(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  29 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(536870911);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 29) ,_mm256_slli_epi32( w1 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 23) ,_mm256_slli_epi32( w1 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 17) ,_mm256_slli_epi32( w1 , 15 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 14) ,_mm256_slli_epi32( w0 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 11) ,_mm256_slli_epi32( w1 , 21 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 8) ,_mm256_slli_epi32( w0 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 5) ,_mm256_slli_epi32( w1 , 27 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 2) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 31) ,_mm256_slli_epi32( w0 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 25) ,_mm256_slli_epi32( w0 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 19) ,_mm256_slli_epi32( w0 , 13 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 13) ,_mm256_slli_epi32( w0 , 19 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 10) ,_mm256_slli_epi32( w1 , 22 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 7) ,_mm256_slli_epi32( w0 , 25 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 4) ,_mm256_slli_epi32( w1 , 28 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta =  _mm256_and_si256 ( mask, _mm256_srli_epi32( w1 , 1) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 27) ,_mm256_slli_epi32( w1 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 21) ,_mm256_slli_epi32( w1 , 11 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 18) ,_mm256_slli_epi32( w0 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 15) ,_mm256_slli_epi32( w1 , 17 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 12) ,_mm256_slli_epi32( w0 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 9) ,_mm256_slli_epi32( w1 , 23 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 6) ,_mm256_slli_epi32( w0 , 26 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 3) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 30-bit values, touching 30 256-bit words, using 480 bytes */ 
static void avxiunpackblock30(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  30 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(1073741823);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 30) ,_mm256_slli_epi32( w1 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 26) ,_mm256_slli_epi32( w1 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 22) ,_mm256_slli_epi32( w1 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 18) ,_mm256_slli_epi32( w1 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 14) ,_mm256_slli_epi32( w1 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 12) ,_mm256_slli_epi32( w0 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 10) ,_mm256_slli_epi32( w1 , 22 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 8) ,_mm256_slli_epi32( w0 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 6) ,_mm256_slli_epi32( w1 , 26 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 4) ,_mm256_slli_epi32( w0 , 28 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 2) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w1 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 28) ,_mm256_slli_epi32( w1 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 24) ,_mm256_slli_epi32( w1 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 22) ,_mm256_slli_epi32( w0 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 20) ,_mm256_slli_epi32( w1 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 18) ,_mm256_slli_epi32( w0 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 16) ,_mm256_slli_epi32( w1 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 14) ,_mm256_slli_epi32( w0 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 12) ,_mm256_slli_epi32( w1 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 10) ,_mm256_slli_epi32( w0 , 22 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 8) ,_mm256_slli_epi32( w1 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 6) ,_mm256_slli_epi32( w0 , 26 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 4) ,_mm256_slli_epi32( w1 , 28 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w1 , 2) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 31-bit values, touching 31 256-bit words, using 496 bytes */ 
static void avxiunpackblock31(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  31 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  const __m256i mask = _mm256_set1_epi32(2147483647);
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  w0 ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 31) ,_mm256_slli_epi32( w1 , 1 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 30) ,_mm256_slli_epi32( w0 , 2 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 29) ,_mm256_slli_epi32( w1 , 3 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 28) ,_mm256_slli_epi32( w0 , 4 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 27) ,_mm256_slli_epi32( w1 , 5 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 26) ,_mm256_slli_epi32( w0 , 6 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 25) ,_mm256_slli_epi32( w1 , 7 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 24) ,_mm256_slli_epi32( w0 , 8 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 23) ,_mm256_slli_epi32( w1 , 9 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 22) ,_mm256_slli_epi32( w0 , 10 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 21) ,_mm256_slli_epi32( w1 , 11 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 20) ,_mm256_slli_epi32( w0 , 12 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 19) ,_mm256_slli_epi32( w1 , 13 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 18) ,_mm256_slli_epi32( w0 , 14 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 17) ,_mm256_slli_epi32( w1 , 15 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 16) ,_mm256_slli_epi32( w0 , 16 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 15) ,_mm256_slli_epi32( w1 , 17 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 14) ,_mm256_slli_epi32( w0 , 18 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 13) ,_mm256_slli_epi32( w1 , 19 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 12) ,_mm256_slli_epi32( w0 , 20 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 11) ,_mm256_slli_epi32( w1 , 21 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 10) ,_mm256_slli_epi32( w0 , 22 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 9) ,_mm256_slli_epi32( w1 , 23 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 8) ,_mm256_slli_epi32( w0 , 24 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 7) ,_mm256_slli_epi32( w1 , 25 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 6) ,_mm256_slli_epi32( w0 , 26 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 5) ,_mm256_slli_epi32( w1 , 27 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 4) ,_mm256_slli_epi32( w0 , 28 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w0 , 3) ,_mm256_slli_epi32( w1 , 29 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  _mm256_and_si256 ( mask,  _mm256_or_si256 (_mm256_srli_epi32( w1 , 2) ,_mm256_slli_epi32( w0 , 30 ) ) ) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  delta = _mm256_srli_epi32( w0 , 1) ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


/* we packed 256 32-bit values, touching 32 256-bit words, using 512 bytes */ 
static void avxiunpackblock32(const __m256i * compressed, uint32_t * pout, uint32_t init_value) {
  /* we are going to access  32 256-bit words */ 
  __m256i w0, w1;
  __m256i * out = (__m256i *) pout;
  __m256i prev = _mm256_set1_epi32(init_value);
  __m256i delta;
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w0 = _mm256_lddqu_si256(compressed++);
  delta =  w0 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
  w1 = _mm256_lddqu_si256(compressed++);
  delta =  w1 ;
  prev = avx_prefix_sum(delta, prev);
  _mm256_storeu_si256(out++, prev);
}


void avxpackwithoutmaskd1(uint32_t initvalue, const uint32_t *in, __m256i *out, uint32_t bit) {
    switch (bit) {
    case 0: avxipackblock0(in, out, initvalue); break;
    case 1: avxipackblock1(in, out, initvalue); break;
    case 2: avxipackblock2(in, out, initvalue); break;
    case 3: avxipackblock3(in, out, initvalue); break;
    case 4: avxipackblock4(in, out, initvalue); break;
    case 5: avxipackblock5(in, out, initvalue); break;
    case 6: avxipackblock6(in, out, initvalue); break;
    case 7: avxipackblock7(in, out, initvalue); break;
    case 8: avxipackblock8(in, out, initvalue); break;
    case 9: avxipackblock9(in, out, initvalue); break;
    case 10: avxipackblock10(in, out, initvalue); break;
    case 11: avxipackblock11(in, out, initvalue); break;
    case 12: avxipackblock12(in, out, initvalue); break;
    case 13: avxipackblock13(in, out, initvalue); break;
    case 14: avxipackblock14(in, out, initvalue); break;
    case 15: avxipackblock15(in, out, initvalue); break;
    case 16: avxipackblock16(in, out, initvalue); break;
    case 17: avxipackblock17(in, out, initvalue); break;
    case 18: avxipackblock18(in, out, initvalue); break;
    case 19: avxipackblock19(in, out, initvalue); break;
    case 20: avxipackblock20(in, out, initvalue); break;
    case 21: avxipackblock21(in, out, initvalue); break;
    case 22: avxipackblock22(in, out, initvalue); break;
    case 23: avxipackblock23(in, out, initvalue); break;
    case 24: avxipackblock24(in, out, initvalue); break;
    case 25: avxipackblock25(in, out, initvalue); break;
    case 26: avxipackblock26(in, out, initvalue); break;
    case 27: avxipackblock27(in, out, initvalue); break;
    case 28: avxipackblock28(in, out, initvalue); break;
    case 29: avxipackblock29(in, out, initvalue); break;
    case 30: avxipackblock30(in, out, initvalue); break;
    case 31: avxipackblock31(in, out, initvalue); break;
    case 32: avxipackblock32(in, out, initvalue); break;
    default: break;
    }
}

void avxunpackd1(uint32_t initvalue, const __m256i *in, uint32_t *out, uint32_t bit) {
    switch (bit) {
    case 0: avxiunpackblock0(in, out, initvalue); break;
    case 1: avxiunpackblock1(in, out, initvalue); break;
    case 2: avxiunpackblock2(in, out, initvalue); break;
    case 3: avxiunpackblock3(in, out, initvalue); break;
    case 4: avxiunpackblock4(in, out, initvalue); break;
    case 5: avxiunpackblock5(in, out, initvalue); break;
    case 6: avxiunpackblock6(in, out, initvalue); break;
    case 7: avxiunpackblock7(in, out, initvalue); break;
    case 8: avxiunpackblock8(in, out, initvalue); break;
    case 9: avxiunpackblock9(in, out, initvalue); break;
    case 10: avxiunpackblock10(in, out, initvalue); break;
    case 11: avxiunpackblock11(in, out, initvalue); break;
    case 12: avxiunpackblock12(in, out, initvalue); break;
    case 13: avxiunpackblock13(in, out, initvalue); break;
    case 14: avxiunpackblock14(in, out, initvalue); break;
    case 15: avxiunpackblock15(in, out, initvalue); break;
    case 16: avxiunpackblock16(in, out, initvalue); break;
    case 17: avxiunpackblock17(in, out, initvalue); break;
    case 18: avxiunpackblock18(in, out, initvalue); break;
    case 19: avxiunpackblock19(in, out, initvalue); break;
    case 20: avxiunpackblock20(in, out, initvalue); break;
    case 21: avxiunpackblock21(in, out, initvalue); break;
    case 22: avxiunpackblock22(in, out, initvalue); break;
    case 23: avxiunpackblock23(in, out, initvalue); break;
    case 24: avxiunpackblock24(in, out, initvalue); break;
    case 25: avxiunpackblock25(in, out, initvalue); break;
    case 26: avxiunpackblock26(in, out, initvalue); break;
    case 27: avxiunpackblock27(in, out, initvalue); break;
    case 28: avxiunpackblock28(in, out, initvalue); break;
    case 29: avxiunpackblock29(in, out, initvalue); break;
    case 30: avxiunpackblock30(in, out, initvalue); break;
    case 31: avxiunpackblock31(in, out, initvalue); break;
    case 32: avxiunpackblock32(in, out, initvalue); break;
    default: break;
    }
}

#endif /* __AVX2__ */


